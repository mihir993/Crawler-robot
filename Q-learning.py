# Implementation  of Q-learning
# again for 3x3 matrix is the starting point
import numpy as np
import ipdb
from Error import qError
import random
from Dummy import generateDummy
from copy import deepcopy
gama = 0.9  # discount factor assuming to be 0.9
# reward vector is as below
# 0 = up / 1 = down / 2 = left / 3= right
reward = [[{0: None, 1: 0, 2: None, 3: 0},  # state = 1
           {0: None, 1: 0, 2: 0, 3: 0},  # State = 2
           {0: None, 1: 0, 2: 0, 3: None}],  # State = 3
          [{0: 0, 1: 0, 2: None, 3: 0},  # State = 4
           {0: 0, 1: 0, 2: 0, 3: 0},  # State = 5
           {0: 0, 1: 0, 2: 0, 3: None}],  # State = 6
          [{0: 0, 1: None, 2: None, 3: -1},  # State = 7
           {0: 0, 1: None, 2: 1, 3: -1},  # State = 8
           {0: 0, 1: None, 2: 1, 3: None}]]  # State = 9
# Here Q function is also function of state and actions
# Q is definded as matrix of 9 members.. every member is a state..
# containing Q values for all four possible actions
Q = [[[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],  # State1,State2, Stete3
     [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]],  # State4, State5, State6
     [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]]]  # State7, State8, State9
# Every time we do down action from a state we will be in new state which is 3
# member after it. (i.e. from state1 to state4) [This is valid only for 3x3]
# Every time we do up action from a state we will be in new state which is 3
# member before it. (i.e. from state4 to state1) [This is valid only for 3x3]
# Every Right action will take us to state which is 1 member after it. [not valid for state3, state6, state9]
# Every Left action will take us to state which is 1 member before it. [not valid for state1, state4, state7]


def qLearning(Q, reward):
    a = [[None, None, None], [None, None, None],  [None, None, None]]
    size = np.shape(Q)
    Qlast = generateDummy(Q)
    iteration = 0
    while qError(Q, Qlast) > 10**-3 or Q == Qlast:  # or iteration <= 800:
        iteration += 1
        Qlast = deepcopy(Q)
        state = random.randint(1, size[0] * size[1])
        # to retrive raw and column from Nos of state generated by random selector
        temp = state / (size[1] * 1.0)
        if ((temp).is_integer()):
            raw = int(temp) - 1
        else:
            raw = int(temp)
        temp = state % size[1]
        col = temp - 1
        # action selection according to selction of state
        if raw == 0 and col == 0:
            action = random.choice([1, 3])
        elif raw == 0 and col == -1:
            action = random.choice([1, 2])
        elif raw == 0:
            action = random.choice([1, 2, 3])

        elif raw == size[0]-1 and col == 0:
            action = random.choice([0, 3])
        elif raw == size[0]-1 and col == -1:
            action = random.choice([0, 2])
        elif raw == size[0]-1:
            action = random.choice([0, 2, 3])

        elif col == 0:
            action = random.choice([0, 1, 3])
        elif col == -1:
            action = random.choice([0, 1, 2])

        else:
            action = random.randint(0, 3)

        if action == 0:  # Up movement
            nextstate = Q[raw-1][col]
        elif action == 1:  # Down movememt
            nextstate = Q[raw+1][col]
        elif action == 2:  # Left movement
            nextstate = Q[raw][col-1]
        else:  # Right movement
            nextstate = Q[raw][col+1]
        # ipdb.set_trace()
        try:
            Q[raw][col][action] = reward[raw][col][action] + gama * (max(nextstate))
        except TypeError as e:
            print("TypeError")
            # ipdb.set_trace()
    for r in range(0, size[0]):
        for c in range(0, size[1]):
            # ipdb.set_trace()
            a[r][c] = Q[r][c].index(max(Q[r][c]))
    # ipdb.set_trace()
    return Q, a, iteration


trial = qLearning(Q, reward)
print(trial[0])
print("\n")
print(trial[1])
print("\n")
print(trial[2])
